Unit 4: Topic modeling in R
===============================
author: Fred Boehm
date: July 21, 2016
autosize: true


Workshop 4 Learning Objectives
=========

1. Fit LDA models with R code
2. Study the effect of different values of $K$ (number of topics)
3. Compare results across media sources (Twitter vs. New York Times print)




Continuing with our text processing
====

- To re-do the analyses from our first two sessions, we'll use the `source()` function:
  - You may need to change the directory path in the following command:

```{r}
source("../unit1/unit1.R")
source("../unit2/unit2.R")
```

- This runs the entirety of the code in the files "unit1.R" and "unit2.R"

What do we still need to do? 
====

- Goal: A vector for each article, where each word is one element in the vector

We currently have `tx_list2` object: 

```{r}
length(tx_list2)
is.list(tx_list2)
```


Recall an element of tx_list2
====

```{r}
head(tx_list2[[1]])
```

Making a stop list
===

- A stop list is a list of words that we'll remove from our collection of words to be analyzed
- For many analyses, we want to remove pronouns, articles, and other short words

Making a stop list
===

```{r}
library(tm)
stopwords <- c(tm::stopwords("SMART"), "dr", "mr", "ms", "mrs") # add titles to stoplist
```


What is the pipe `%>%`? 
===
- Like a 'pipe' in shell scripting
- Takes the output of one operation and makes it the input of the next operation

```{r}
letters
```

What is the pipe? 
===

```{r}
head(letters)
letters %>% head
```


More processing steps
===

```{r split_and_remove_punctuation}
good2 <- sapply(FUN = function(x)paste(x, collapse = " "), X = tx_list2)  %>%
  stringr::str_split( pattern = " ") 
```

More processing steps
===

```{r}
head(good2[[1]])
```


More processing steps
===

```{r}
good2 <- sapply(FUN = function(x) gsub("'", "", x), X = good2) %>% 
  # remove apostrophes
  sapply(FUN = function(x) gsub("[[:punct:]]", " ", x)) %>%  
  # replace punctuation with space
  sapply(FUN = function(x) gsub("[[:cntrl:]]", " ", x)) %>% 
  # replace control characters with space
  sapply(FUN = function(x) gsub("^[[:space:]]+", "", x)) %>% 
  # remove whitespace at beginning of documents
  sapply(FUN = function(x) gsub("[[:space:]]+$", "", x))  
  # remove whitesp at end
```

More processing steps
===

```{r}
head(good2[[1]])
```



More processing steps
===

```{r}
good2 <-  sapply(FUN = function(x)tolower(x), X = good2) %>%
  sapply(FUN = function(x)x[!(x == "")]) %>% 
  # remove elements that are ""
  sapply(FUN = function(x)x[!(x %in% stopwords)])
# remove stopwords
```

More processing steps
===

```{r}
head(good2[[1]])
```

Processing tweets
===

```{r load-tweets, cache = TRUE}
tweet_dir <- "../data/tweets/"
fns <- dir(tweet_dir)
out <- character(0)
for (file in fns[1]){
  tmp <- read.csv(file.path(tweet_dir, file), stringsAsFactors=FALSE)
  out <- rbind(out, tmp)
}
```

What is the 'out' R object?
===

```{r}
dim(out)
names(out)
```




Remove non-English tweets
===


```{r en-lang-only}
tweets <- out[out$lang == "en",]
```


Process tweets
===


```{r split-tweets-to-words-and-clean}
tw_txt<- tweets$text
library(stringr)
tw_words <- stringr::str_split(tw_txt, pattern = " ") %>% 
  sapply(FUN = function(x)x[!(x == "")]) %>% 
  # remove elements that are ""
  sapply(FUN = function(x) gsub("&amp", "", x)) %>%
  sapply(FUN = function(x) gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", x)) %>%
  sapply(FUN = function(x) gsub("@\\w+", "", x)) %>%
  sapply(FUN = function(x) gsub("[[:punct:]]", "", x)) %>%
  sapply(FUN = function(x) gsub("[[:digit:]]", "", x)) %>% 
  sapply(FUN = function(x) gsub("[^[:graph:]]", "", x)) %>% 
# remove nongraphical chars
# http://www.regular-expressions.info/posixbrackets.html  
  sapply(FUN = function(x) gsub("http\\w+", "", x)) %>% 
  sapply(FUN = function(x) gsub("[ \t]{2,}", "", x)) %>% 
  sapply(FUN = function(x) gsub("^\\s+|\\s+$", "", x)) %>% 
  sapply(FUN = function(x) str_replace_all(x," ","")) 
```

What is `tw_words` R object?
===

```{r}
head(tw_words)
```



More processing of tweets
===

```{r}
tw_words <- sapply(X = tw_words, FUN = function(x) str_replace(x,"RT @[a-z,A-Z]*: ","")) %>% 
  sapply(FUN = function(x) str_replace_all(x,"#[a-z,A-Z]*","")) %>% 
  sapply(FUN = function(x) str_replace_all(x,"@[a-z,A-Z]*","")) %>% 
  # remove whitesp
  sapply(FUN = function(x)tolower(x)) %>%
  sapply(FUN = function(x)x[!(x %in% tm::stopwords("SMART"))]) 
# remove stopwords
# remove "rt" and ""
  tw_words<- sapply(X = tw_words, FUN = function(x)x[!(x %in% c("", "rt"))])
```

What is `tw_words` now?
====

```{r}
head(tw_words)
```



Next Steps
=======

1. Background on topic models
1. LDA models in R
2. Visualizing model results


